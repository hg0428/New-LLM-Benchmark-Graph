{
    "model_name": "Qwen2.5-14B-Instruct",
    "model_family": "qwen2.5",
    "architecture": "Qwen2ForCausalLM",
    "vision": false,
    "inputs": [
        "text"
    ],
    "outputs": [
        "text"
    ],
    "parameters": {
        "total_params": 14800000000,
        "embedding_size": 5120,
        "layers": 48,
        "hidden_size": 5120,
        "intermediate_size": 13824,
        "attention_heads": 40,
        "head_dimensions": 128,
        "vocab_size": 152064
    },
    "benchmarks": {
        "MMLU": 79.7,
        "MMLU Pro": 51.2,
        "MMLU-redux": 76.6,
        "BBH": 78.2,
        "ARC Challenge": 67.3,
        "TruthfulQA": 58.4,
        "Winogrande": null,
        "HellaSwag": null,
        "GPQA": 32.8,
        "Theoremqa": 43.0,
        "MATH": 55.6,
        "MMLU-stem": 76.4,
        "GSM8K": 90.2,
        "HumanEval": 56.7,
        "HumanEval+": 51.2,
        "MBPP": 76.7,
        "MBPP+": 63.2,
        "MultiPL-E": 53.5,
        "Multi-Exam": 70.6,
        "Multi-Understanding": 85.9,
        "Multi-Mathematics": 68.5,
        "Multi-Translation": 36.2
    }
}